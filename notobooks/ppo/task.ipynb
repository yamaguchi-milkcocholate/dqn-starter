{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de00a92-92c3-4f43-a1bb-e208136e575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/root-qv7zuvpL-py3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "import feather\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import *\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 400\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a8d6a6-e40d-4ad2-96ec-38797ebad597",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bfdea5-3a7a-4626-93d8-bfd0628e476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'logprob', 'is_end'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "\n",
    "    def push(self, state: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, logprob: torch.Tensor, is_end: bool) -> None:\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(state, action, reward, logprob, is_end))\n",
    "    \n",
    "    def extend(self, memory: \"ReplayMemory\") -> None:\n",
    "        self.memory.extend(memory.memory)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.memory = deque()\n",
    "    \n",
    "    def get_batches(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        batch = Transition(*zip(*self.memory))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        logprob_batch = torch.cat(batch.logprob)\n",
    "        is_end_batch = np.vstack(batch.is_end)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, logprob_batch, is_end_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d8de0b-a89f-49e5-bed5-e48bed0ffdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, clips: List[Dict[str, Any]]):\n",
    "        super(Actor, self).__init__()\n",
    "        self.clips = clips\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "        self.transforms = {\n",
    "            \"softmax\": nn.Softmax(dim=1),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        for clip in self.clips:\n",
    "            x[:, clip[\"dims\"]] = clip[\"coef\"] * self.transforms[clip[\"name\"]](x[:, clip[\"dims\"]])\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, device: torch.device, spread: float):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mean_dim = action_dim\n",
    "        self.cov_dim = action_dim\n",
    "        # self.cov_dim = (action_dim + 1) * action_dim // 2\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.actor = Actor(\n",
    "            input_dim=state_dim, \n",
    "            output_dim=self.mean_dim + self.cov_dim,\n",
    "            clips=[\n",
    "                {\"dims\": [0], \"coef\": spread, \"name\": \"tanh\"},\n",
    "                # {\"dims\": [1, 2, 3], \"coef\": 1, \"name\": \"softmax\"},\n",
    "                {\"dims\": [4], \"coef\": spread / 2, \"name\": \"tanh\"},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def act(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        dist = self._get_normal_dist(state=state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        return action.detach(), dist.log_prob(action).detach()\n",
    "    \n",
    "    def deterministical_act(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        embed = self.actor(state)\n",
    "        _mean = embed[:, :self.action_dim]\n",
    "        return _mean.detach()\n",
    "    \n",
    "    def evaluate(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        dist = self._get_normal_dist(state=state)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "        \n",
    "    def _get_normal_dist(self, state: torch.Tensor) -> torch.distributions.MultivariateNormal:\n",
    "        embed = self.actor(state)\n",
    "\n",
    "        _mean = embed[:, :self.action_dim]\n",
    "        _cov_as_vec = embed[0, self.action_dim:]\n",
    "        _cov = torch.diag(_cov_as_vec) * 1e-3 ###\n",
    "#         _cov = torch.zeros((self.action_dim, self.action_dim), dtype=torch.float32)\n",
    "        \n",
    "#         tmp = 0\n",
    "#         for i in range(self.action_dim):\n",
    "#             for j in range(i, self.action_dim):\n",
    "#                 if i == j:\n",
    "#                     _cov[i, j] = self.softplus(_cov_as_vec[tmp + j])\n",
    "#                 else:\n",
    "#                     _cov[i, j] = self.softplus(_cov_as_vec[tmp + j])\n",
    "#             tmp += (self.action_dim - i - 1)\n",
    "        _cov = torch.mm(_cov, torch.t(_cov))    \n",
    "        return torch.distributions.MultivariateNormal(loc=_mean, covariance_matrix=_cov)\n",
    "    \n",
    "\n",
    "class PPO(object):\n",
    "    def __init__(self, state_dim: int, action_dim: int, device: torch.device, params: Dict[str, Any]):\n",
    "        self.device = device\n",
    "        \n",
    "        self.gamma = params[\"GAMMA\"]\n",
    "        self.eps_clip = params[\"EPS_CLIP\"]\n",
    "        self.K_epochs = params[\"K_EPOCHS\"]\n",
    "        \n",
    "        self.memory = ReplayMemory()\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim=state_dim, action_dim=action_dim, device=device, spread=params[\"SPREAD\"]).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {\"params\": self.policy.actor.parameters(), \"lr\": params[\"LR_ACTOR\"]},\n",
    "            {\"params\": self.policy.critic.parameters(), \"lr\": params[\"LR_CRITIC\"]},\n",
    "        ])\n",
    "        \n",
    "        self.policy_old = ActorCritic(state_dim=state_dim, action_dim=action_dim, device=device, spread=params[\"SPREAD\"]).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[torch.Tensor, float]:\n",
    "        with torch.no_grad():\n",
    "            action, action_logprob = self.policy_old.act(state=state)\n",
    "            return action, action_logprob\n",
    "    \n",
    "    def select_deterministical_action(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            action = self.policy_old.deterministical_act(state=state)\n",
    "            return action\n",
    "        \n",
    "    def store_transition(\n",
    "        self, \n",
    "        state: torch.Tensor, \n",
    "        action: torch.Tensor,\n",
    "        reward: torch.Tensor, \n",
    "        logprob: torch.Tensor,\n",
    "        is_end: bool,\n",
    "    ) -> None:\n",
    "        self.memory.push(state=state, action=action, reward=reward, logprob=logprob, is_end=is_end)\n",
    "    \n",
    "    def merge_memories(self, memories: List[ReplayMemory]) -> None:\n",
    "        self.memory = ReplayMemory()\n",
    "        for _mem in memories:\n",
    "            self.memory.extend(_mem)\n",
    "    \n",
    "    def update(self):\n",
    "        state_batch, action_batch, reward_batch, logprob_batch, is_end_batch = self.memory.get_batches()\n",
    "        \n",
    "        old_rewards = self._calc_cumsum_discount_rewards(reward_batch=reward_batch, is_end_batch=is_end_batch)\n",
    "        \n",
    "        old_states = state_batch.detach().to(self.device)\n",
    "        old_actions = action_batch.detach().to(self.device)\n",
    "        old_logprobs = logprob_batch.detach().to(self.device)\n",
    "        \n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(state=old_states, action=old_actions)\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            \n",
    "            advantages = old_rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            \n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, old_rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.memory.clear()\n",
    "    \n",
    "    def _calc_cumsum_discount_rewards(self, reward_batch: torch.Tensor, is_end_batch: np.ndarray) -> torch.Tensor:\n",
    "        reward_batch = reward_batch.squeeze()\n",
    "        is_end_batch = is_end_batch.squeeze()\n",
    "        cumsum_discounted_rewards = deque()\n",
    "        _discounted_reward = 0\n",
    "        for reward, is_end in zip(reversed(reward_batch), reversed(is_end_batch)):\n",
    "            if is_end:\n",
    "                _discounted_reward = 0\n",
    "            _discounted_reward = reward + self.gamma * _discounted_reward\n",
    "            cumsum_discounted_rewards.appendleft(_discounted_reward)\n",
    "        cumsum_discounted_rewards = torch.tensor(cumsum_discounted_rewards, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        def normalize(x: torch.Tensor) -> torch.Tensor:\n",
    "            return (x - x.mean()) / (x.std() + 1e-7)\n",
    "                \n",
    "        return normalize(x=cumsum_discounted_rewards)\n",
    "    \n",
    "    def save_model(self, filepath: Path) -> None:\n",
    "        torch.save(self.policy_old.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb930b0d-251d-4e05-b577-42203a50e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "datadir = rootdir / \"data\" / \"bybit\" / \"2022-07-24\"\n",
    "cachedir = rootdir / \"data\" / \"cache\" / \"ppo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4b29c1-f48f-4605-a84e-a941676415c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "      <th>max_price</th>\n",
       "      <th>min_price</th>\n",
       "      <th>buy_price</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1609426800</td>\n",
       "      <td>28490</td>\n",
       "      <td>28490</td>\n",
       "      <td>28339</td>\n",
       "      <td>28490</td>\n",
       "      <td>28339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1609427100</td>\n",
       "      <td>28569</td>\n",
       "      <td>28569</td>\n",
       "      <td>28490</td>\n",
       "      <td>28569</td>\n",
       "      <td>28490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1609427400</td>\n",
       "      <td>28622</td>\n",
       "      <td>28623</td>\n",
       "      <td>28539</td>\n",
       "      <td>28623</td>\n",
       "      <td>28539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1609427700</td>\n",
       "      <td>28559</td>\n",
       "      <td>28626</td>\n",
       "      <td>28532</td>\n",
       "      <td>28626</td>\n",
       "      <td>28532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1609428000</td>\n",
       "      <td>28598</td>\n",
       "      <td>28620</td>\n",
       "      <td>28559</td>\n",
       "      <td>28620</td>\n",
       "      <td>28559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  price  max_price  min_price  buy_price  sell_price\n",
       "0  1609426800  28490      28490      28339      28490       28339\n",
       "1  1609427100  28569      28569      28490      28569       28490\n",
       "2  1609427400  28622      28623      28539      28623       28539\n",
       "3  1609427700  28559      28626      28532      28626       28532\n",
       "4  1609428000  28598      28620      28559      28620       28559"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = list()\n",
    "for i in range(1, 10):\n",
    "    dfs.append(feather.read_dataframe(datadir / f\"data_{i}.feather\"))\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "del dfs\n",
    "gc.collect()\n",
    "\n",
    "df = df[[\"open_time\", \"close\", \"high\", \"low\"]].astype(float).astype(int)\n",
    "\n",
    "df.columns = [\"timestamp\", \"price\", \"max_price\", \"min_price\"]\n",
    "df[[\"buy_price\", \"sell_price\"]] = df[[\"max_price\", \"min_price\"]]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d775575-f08f-4160-9401-91db7e46387b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[\"_diff\"] = df[\"price\"].diff()\n",
    "    df[\"spread_upper\"] = df[\"max_price\"] / df[\"price\"] - 1\n",
    "    df[\"spread_lower\"] = df[\"min_price\"] / df[\"price\"] - 1\n",
    "    \n",
    "    for minutes in [1, 2]:\n",
    "        nm_dsharp, nm_p, nm_pcs, nm_area, nm_maxval, nm_minval, nm_maxlen, nm_minlen, nm_change = [\n",
    "            f\"{nm}_{minutes}\" for nm in [\"dsharp\", \"_p\", \"_pcs\", \"area\", \"maxval\", \"minval\", \"maxlen\", \"minlen\", \"change\"]\n",
    "        ]\n",
    "        \n",
    "        # 微分Sharp比\n",
    "        df[nm_dsharp] = df[\"_diff\"].rolling(minutes * 6).mean() / (df[\"_diff\"].rolling(minutes * 6).std() + 1.0)\n",
    "\n",
    "        df[nm_p] = find_cross_zero(x=df[nm_dsharp].values)\n",
    "        df[nm_pcs] = df[nm_p].cumsum()\n",
    "\n",
    "        _values = np.empty((df.shape[0], 4))\n",
    "        for i, (price, ds, p, pcs) in enumerate(df[[\"price\", nm_dsharp, nm_p, nm_pcs]].values):    \n",
    "            sign = np.sign(ds)\n",
    "            if pcs == 0:\n",
    "                mt = {\n",
    "                    nm_area: np.nan, nm_maxval: np.nan, nm_minval: np.nan,\n",
    "                    nm_maxlen: np.nan, nm_minlen: np.nan,\n",
    "                }\n",
    "            else:\n",
    "                if p:\n",
    "                    mt = {\n",
    "                        nm_area: 0, nm_maxval: -np.inf, nm_minval: np.inf,\n",
    "                        nm_maxlen: 0, nm_minlen: 0,\n",
    "                    }\n",
    "                mt[nm_area] += sign * ds\n",
    "                if ds > mt[nm_maxval]:\n",
    "                    mt[nm_maxlen] = 0\n",
    "                    mt[nm_maxval] = ds\n",
    "                else:\n",
    "                    mt[nm_maxlen] += 1\n",
    "                if ds < mt[nm_minval]:\n",
    "                    mt[nm_minlen] = 0\n",
    "                    mt[nm_minval] = ds\n",
    "                else:\n",
    "                    mt[nm_minlen] += 1\n",
    "            _values[i] = np.array([\n",
    "                mt[nm_area],\n",
    "                max(sign * mt[nm_maxval], sign * mt[nm_minval]),\n",
    "                mt[nm_maxlen],\n",
    "                mt[nm_minlen],\n",
    "            ])\n",
    "        df[[nm_area, nm_change, nm_maxlen, nm_minlen]] = _values\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_cross_zero(x: np.ndarray) -> np.ndarray:\n",
    "    x_len = x.shape[0] - 1\n",
    "    y = np.zeros(x.shape[0]).astype(bool)\n",
    "    for i in range(x.shape[0] - 1):\n",
    "        if (x[x_len - i] > 0) and x[x_len - i - 1] <= 0:\n",
    "            y[x_len - i] = True\n",
    "        elif (x[x_len - i] < 0) and x[x_len - i - 1] >= 0:\n",
    "            y[x_len - i] = True\n",
    "        else:\n",
    "            y[x_len - i] = False\n",
    "    return y\n",
    "\n",
    "\n",
    "def equal_divide_indice(length, num_divide):\n",
    "    x = np.linspace(0, length - 1, length)\n",
    "    indice = np.ones_like(x) * -1\n",
    "    for i, thresh in enumerate(np.linspace(0, length, num_divide + 1)[:-1].astype(int)):\n",
    "        indice[thresh : ] = i\n",
    "    return indice\n",
    "\n",
    "\n",
    "def divide_with_pcs(df, num_divide, division):\n",
    "    df[\"_eq_fold\"] = equal_divide_indice(length=df.shape[0], num_divide=num_divide)\n",
    "    df[\"fold\"] = np.nan\n",
    "    for i, (start, end) in enumerate(df.groupby(\"_eq_fold\")[division].agg([\"min\", \"max\"]).values):\n",
    "        indice = (start < df[division]) & (df[division] <= end)\n",
    "        df.loc[indice, \"fold\"] = i\n",
    "    df[\"fold\"] = df[\"fold\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ef3481-f2e0-492f-a2d9-aee337e736f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df: pd.DataFrame, features: List[str], lags: List[int]) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    features_with_lags = [] + features\n",
    "    for lag in lags:\n",
    "        lag_features = [f\"{f}_lag{lag}\" for f in features]\n",
    "        df[lag_features] = df[features].shift(lag)\n",
    "        features_with_lags += lag_features\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df, features_with_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1272f1-a401-4de7-be34-69b99197d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['dsharp_1', 'area_1','change_1', 'maxlen_1', 'minlen_1', 'dsharp_2', 'area_2', 'change_2', 'maxlen_2', 'minlen_2', 'spread_upper', 'spread_lower']\n",
    "\n",
    "dfa = add_features(df=df)\n",
    "dfa, features = add_lag_features(df=dfa, features=features, lags=[1, 2, 3])\n",
    "\n",
    "train = divide_with_pcs(df=dfa, num_divide=5, division=\"_pcs_2\")\n",
    "train = train[train.columns[~train.columns.str.startswith(\"_\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c885035-fc1f-4dbd-bb26-3496b176fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Market(object):    \n",
    "    def __init__(self, df: pd.DataFrame, features: List[str], is_eval: bool = False):\n",
    "        self.market_states_cols = features\n",
    "        self.market_states = df[self.market_states_cols].values\n",
    "        self.is_eval = is_eval\n",
    "        self.prices = df[[\"price\", \"max_price\", \"min_price\", \"buy_price\", \"sell_price\"]].values\n",
    "        self.i = 0\n",
    "        \n",
    "        self.ob, self.os = False, False\n",
    "        self.fb, self.fs = False, False\n",
    "        self.lb, self.ls = None, None\n",
    "        self.step_from_ob, self.step_from_os = 0, 0\n",
    "        self.step_from_fb, self.step_from_fs = 0, 0\n",
    "        \n",
    "        self.sum_rtn = 0\n",
    "        self.rtns = list()\n",
    "        self.cur_rtn_sum = 0\n",
    "        self.prices_when_fill = deque()\n",
    "        self.position_side = None\n",
    "        \n",
    "        self.is_transaction_end = False\n",
    "    \n",
    "    @property\n",
    "    def num_steps(self) -> int:\n",
    "        return self.prices.shape[0] - 2\n",
    "            \n",
    "    def step(self, logits: np.ndarray) -> Tuple[float, bool]:        \n",
    "        def sample_categorical(logits: np.ndarray) -> str:\n",
    "            logits = logits - logits.max()\n",
    "            probs = np.exp(logits) / np.exp(logits).sum()\n",
    "            return np.random.choice([\"Hold\", \"Buy\", \"Sell\", \"Cancel\"], p=probs)\n",
    "        \n",
    "        spread = logits[0]\n",
    "        if self.is_eval:\n",
    "            action = [\"Hold\", \"Buy\", \"Sell\", \"Cancel\"][logits[1:].argmax()]\n",
    "        else:\n",
    "            action = sample_categorical(logits=logits[1:])\n",
    "        \n",
    "        market_state = self.market_states[self.i]\n",
    "        price, _, _ = self.prices[self.i, [0, 3, 4]]\n",
    "        sthprice, bthprice = self.prices[self.i + 1, [1, 2]]\n",
    "        \n",
    "        if action == \"Buy\":\n",
    "            if self.fb:\n",
    "                pass\n",
    "            else:\n",
    "                if self.os:\n",
    "                    self.unset_order_sell()\n",
    "                self.set_order_buy(price=price * (1 - spread))\n",
    "        elif action == \"Sell\":\n",
    "            if self.fs:\n",
    "                pass\n",
    "            else:\n",
    "                if self.ob:\n",
    "                    self.unset_order_buy()\n",
    "                self.set_order_sell(price=price * (1 + spread))\n",
    "        elif action == \"Hold\":\n",
    "            pass\n",
    "        elif action == \"Cancel\":\n",
    "            if self.ob:\n",
    "                self.unset_order_buy()\n",
    "            elif self.os:\n",
    "                self.unset_order_sell()\n",
    "        \n",
    "        if self.ob:\n",
    "            self.step_from_ob += 1\n",
    "            if self.lb > bthprice:\n",
    "                self.fb = True\n",
    "                self.ob = False\n",
    "                self.step_from_ob = 0\n",
    "                self.position_side = \"Buy\"\n",
    "        \n",
    "        if self.os:\n",
    "            self.step_from_os += 1\n",
    "            if self.ls < sthprice:\n",
    "                self.fs = True\n",
    "                self.os = False\n",
    "                self.step_from_os = 0\n",
    "                self.position_side = \"Sell\"\n",
    "        \n",
    "        if self.fb:\n",
    "            self.step_from_fb += 1\n",
    "        \n",
    "        if self.fs:\n",
    "            self.step_from_fs += 1\n",
    "            \n",
    "        if (self.lb is not None) and (self.ls is not None):\n",
    "            cur_rtn = self.ls / self.lb - 1\n",
    "            self.append_sharp_ration_diff()\n",
    "        elif self.lb is not None:\n",
    "            cur_rtn = price / self.lb - 1\n",
    "            self.append_sharp_ration_diff()\n",
    "        elif self.ls is not None:\n",
    "            cur_rtn = self.ls / price - 1\n",
    "            self.append_sharp_ration_diff()\n",
    "        else:\n",
    "            cur_rtn = 0\n",
    "        self.cur_rtn_sum += cur_rtn\n",
    "        sharp_ratio = self.calc_sharp_ratio()\n",
    "        \n",
    "        if self.fb and self.fs:\n",
    "            self.step_from_fb, self.step_from_fs = 0, 0\n",
    "            rtn = self.ls / self.lb - 1\n",
    "            self.fb, self.fs = False, False\n",
    "            self.sum_rtn += rtn\n",
    "            self.cur_rtn_sum = 0\n",
    "            self.prices_when_fill = deque()\n",
    "            self.position_side = None\n",
    "            self.is_transaction_end = True\n",
    "        else:\n",
    "            rtn = 0\n",
    "        \n",
    "        self.rtns.append({\n",
    "            \"i\": self.i, \"rtn\": self.sum_rtn, \"cur_rtn\": cur_rtn,\n",
    "            \"ob\": self.ob, \"os\": self.os, \"fb\": self.fb, \"fs\": self.fs,\n",
    "            \"act\": action, \"spread\": spread\n",
    "        })\n",
    "        self.i += 1\n",
    "        return sharp_ratio, self.is_transaction_end\n",
    "    \n",
    "    def append_sharp_ration_diff(self):\n",
    "        if len(self.prices_when_fill) == 0:\n",
    "            self.prices_when_fill.append(0)\n",
    "        else:\n",
    "            if self.position_side == \"Buy\":\n",
    "                self.prices_when_fill.append(self.prices[self.i, 0] / self.prices[self.i - 1, 0] - 1)\n",
    "            elif self.position_side == \"Sell\":\n",
    "                self.prices_when_fill.append(self.prices[self.i - 1, 0] / self.prices[self.i, 0] - 1)\n",
    "    \n",
    "    def calc_sharp_ratio(self) -> float:\n",
    "        if len(self.prices_when_fill) > 2:\n",
    "            rtn = np.mean(self.prices_when_fill)\n",
    "            sigma = np.std(self.prices_when_fill)\n",
    "\n",
    "            if sigma == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return rtn / sigma - 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def set_order_buy(self, price):\n",
    "        self.ob = True\n",
    "        self.lb = price\n",
    "        self.step_from_ob = 0\n",
    "    \n",
    "    def set_order_sell(self, price):\n",
    "        self.os = True\n",
    "        self.ls = price\n",
    "        self.step_from_os = 0\n",
    "    \n",
    "    def unset_order_buy(self):\n",
    "        self.ob = False\n",
    "        self.lb = None\n",
    "        self.step_from_ob = 0\n",
    "    \n",
    "    def unset_order_sell(self):\n",
    "        self.os = False\n",
    "        self.ls = None\n",
    "        self.step_from_os = 0\n",
    "        \n",
    "    def state(self) -> np.ndarray:\n",
    "        if (self.lb is not None) and (self.ls is not None):\n",
    "            cur_rtn = self.ls / self.lb - 1\n",
    "        elif self.lb is not None:\n",
    "            cur_rtn = self.prices[self.i, 0] / self.lb - 1\n",
    "        elif self.ls is not None:\n",
    "            cur_rtn = self.ls / self.prices[self.i, 0] - 1\n",
    "        else:\n",
    "            cur_rtn = 0\n",
    "        trade_state = np.array([\n",
    "            int(self.ob), int(self.os), int(self.fb), int(self.fs), \n",
    "            cur_rtn,\n",
    "            np.log1p(self.step_from_ob), np.log1p(self.step_from_os), np.log1p(self.step_from_fb), np.log1p(self.step_from_fs),\n",
    "        ])\n",
    "        market_state = self.market_states[self.i]\n",
    "        return np.hstack([trade_state, market_state])\n",
    "    \n",
    "    def get_return(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.rtns)\n",
    "    \n",
    "\n",
    "def random_market(df: pd.DataFrame, features: List[str], num_steps: int, is_eval: bool = False):\n",
    "    idx = np.random.randint(df.shape[0] - 2 - num_steps)\n",
    "    return Market(df=df.iloc[idx : idx + num_steps].reset_index(drop=True), features=features, is_eval=is_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0b2106-606b-4e7c-bc44-0ad8727efa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x: List[float], filepath: Path, xlabel: str = \"\", ylabel: str = \"\"):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.plot(x)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    fig.savefig(filepath)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279721c-eaae-46d0-b2c3-f2f9b215f0b8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62260efe-5746-44ec-9fbe-2932683084e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_params = {\n",
    "    \"GAMMA\": 0.999,\n",
    "    \"EPS_CLIP\": 0.2,\n",
    "    \"K_EPOCHS\": 80,\n",
    "    \"LR_ACTOR\": 0.0003,\n",
    "    \"LR_CRITIC\": 0.001,\n",
    "    \"SPREAD\": 0.002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b914c48-1a6c-4a8d-94fc-4eb241100109",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\"Hold\", \"Buy\", \"Sell\", \"Cancel\"])\n",
    "n_actions = 1 + actions.shape[0]\n",
    "state_dim = 9 + len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b7b24c-8ec3-481d-9c26-050c4d26bf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131193, 55), (32779, 55))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_eval = train.loc[train[\"fold\"] != 4].reset_index(drop=True), train.loc[train[\"fold\"] == 4].reset_index(drop=True)\n",
    "df_train.shape, df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c05b49d6-5ed7-46ea-a968-32b0080ecdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "NUM_STEPS = 12 * 24 * 1 #1ヶ月\n",
    "\n",
    "# TRAIN_LOG_INTERVAL = 1\n",
    "EVAL_LOG_INTERVAL = 25\n",
    "\n",
    "UPDATE_INTERVAL = 5\n",
    "\n",
    "savedir = cachedir / \"task\"\n",
    "savedir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "893fec6c-4b71-4fe2-a313-a3cb803c8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_episode(agent: PPO, market: Market, device: torch.device) -> Tuple[ReplayMemory, deque]:\n",
    "    memory = ReplayMemory()\n",
    "    rewards = deque()\n",
    "    for t in range(market.num_steps):\n",
    "        state = torch.tensor(market.state(), device=device, dtype=torch.float32).view(1, -1)\n",
    "        \n",
    "        action, action_logprob = agent.select_action(state=state)\n",
    "        \n",
    "        reward, is_end = market.step(logits=action.detach().numpy().squeeze())\n",
    "        rewards.append(reward)\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "        \n",
    "        memory.push(state=state, action=action, reward=reward, logprob=action_logprob, is_end=is_end)\n",
    "        \n",
    "        if is_end:\n",
    "            break\n",
    "    return memory, np.mean(rewards)\n",
    "\n",
    "def run_episode(args):\n",
    "    return _run_episode(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6b7c6-271b-4e6d-95fc-b1bbe8a80da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████                                                                                   | 778/10000 [25:54<11:38:12,  4.54s/it]"
     ]
    }
   ],
   "source": [
    "agent = PPO(state_dim=state_dim, action_dim=n_actions, device=device, params=ppo_params)\n",
    "\n",
    "num_async = multiprocessing.cpu_count() - 1\n",
    "pool = multiprocessing.Pool(num_async)\n",
    "\n",
    "log = list()\n",
    "best_reward = -np.inf\n",
    "episode_durations = []\n",
    "for i_episode in tqdm(range(NUM_EPISODES)):\n",
    "    params = list()\n",
    "    for _ in range(num_async):\n",
    "        params.append({\n",
    "            \"agent\": agent, \n",
    "            \"market\": random_market(df=df_train, features=features, num_steps=NUM_STEPS),\n",
    "            \"device\": device,\n",
    "        })\n",
    "    result = pool.map(run_episode, params)\n",
    "    memories, rewards = [r[0] for r in result], [r[1] for r in result]\n",
    "    \n",
    "    episode_durations += rewards\n",
    "    train_reward = np.mean(rewards)\n",
    "    agent.merge_memories(memories=memories)\n",
    "    \n",
    "    del memories, rewards\n",
    "    gc.collect()\n",
    "        \n",
    "    if (i_episode + 1) % UPDATE_INTERVAL == 0:\n",
    "        agent.update()\n",
    "    \n",
    "    plot(x=episode_durations, filepath=savedir / \"episode_durations.png\", xlabel=\"episode\", ylabel=\"avg reward\")\n",
    "    \n",
    "    # train_returns = market.get_return()[\"rtn\"].values\n",
    "    # if (i_episode + 1) % TRAIN_LOG_INTERVAL == 0:\n",
    "    #     plot(x=train_returns, filepath=savedir / f\"train_returns_{i_episode + 1}.png\", xlabel=\"steps\", ylabel=\"return\")\n",
    "    #     plot(x=rewards, filepath=savedir / f\"train_reward_{i_episode + 1}.png\", xlabel=\"steps\", ylabel=\"reward\")\n",
    "    #     market.get_return().to_csv(savedir / f\"train_log_{i_episode + 1}.csv\", index=False)\n",
    "    #     agent.save_model(filepath=savedir / f\"model_{i_episode + 1}.pt\")\n",
    "\n",
    "    if (i_episode + 1) % EVAL_LOG_INTERVAL == 0:\n",
    "        market = random_market(df=df_eval, features=features, num_steps=NUM_STEPS, is_eval=True)\n",
    "        \n",
    "        rewards = list()\n",
    "        for t in range(market.num_steps):\n",
    "            state = torch.tensor(market.state(), device=device, dtype=torch.float32).view(1, -1)\n",
    "            \n",
    "            action = agent.select_deterministical_action(state=state)\n",
    "            \n",
    "            reward, is_end = market.step(logits=action.detach().numpy().squeeze())\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        eval_returns = market.get_return()[\"rtn\"].values\n",
    "        plot(x=eval_returns, filepath=savedir / f\"eval_returns_{i_episode + 1}.png\", xlabel=\"steps\", ylabel=\"return\")\n",
    "        plot(x=rewards, filepath=savedir / f\"eval_reward_{i_episode + 1}.png\", xlabel=\"steps\", ylabel=\"reward\")\n",
    "        market.get_return().to_csv(savedir / f\"eval_log_{i_episode + 1}.csv\", index=False)\n",
    "        \n",
    "        eval_reward = np.mean(rewards)\n",
    "        if eval_reward > best_reward:\n",
    "            plot(x=rewards, filepath=savedir / \"eval_reward_best.png\", xlabel=\"steps\", ylabel=\"reward\")\n",
    "            plot(x=eval_returns, filepath=savedir / \"eval_returns_best.png\", xlabel=\"steps\", ylabel=\"return\")\n",
    "            agent.save_model(filepath=savedir / \"model_best.pt\")\n",
    "            best_reward = eval_reward\n",
    "        \n",
    "        del market\n",
    "        gc.collect()\n",
    "    else:\n",
    "        eval_returns = [np.nan]\n",
    "        eval_reward = np.nan\n",
    "    \n",
    "    log.append({\n",
    "        \"episode\": i_episode + 1, \"train_reward\": train_reward, \"train_return\": None,\n",
    "        \"eval_reward\": eval_reward, \"eval_return\": eval_returns[-1]\n",
    "    })\n",
    "    gc.collect()\n",
    "\n",
    "del agent, pool\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e85de-2fcc-414a-a1bc-c4fc537d3981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
