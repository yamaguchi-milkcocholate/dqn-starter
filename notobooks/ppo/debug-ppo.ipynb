{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9d6825-6fb7-4e2d-af4e-18545e9abeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/root-qv7zuvpL-py3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy\n",
    "from stable_baselines3.common.utils import get_schedule_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c290227f-a020-4212-872c-d44784354fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.state_dim = 5\n",
    "        self.action_dim = 3\n",
    "        self.action_space = gym.spaces.Discrete(self.action_dim) \n",
    "        \n",
    "        #状態が3つの時で上限と下限の設定と仮定\n",
    "        LOW = np.zeros(self.state_dim).astype(float)\n",
    "        HIGH = np.ones(self.state_dim).astype(float)\n",
    "        self.observation_space = gym.spaces.Box(low=LOW, high=HIGH)\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_steps = 0\n",
    "        observation = np.random.rand(self.state_dim)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action_index: int):\n",
    "        self.n_steps += 1\n",
    "        if action_index == 1:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        observation = np.random.rand(self.state_dim)\n",
    "        done = self.n_steps > 10\n",
    "        return observation, reward, done, {\"episode\": {\"r\": 1, \"l\": 1}}\n",
    "\n",
    "    def render(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e588fb4-8fbf-4cbb-9877-7027704c731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 5\n",
    "action_dim = 3\n",
    "\n",
    "\n",
    "state_space = gym.spaces.Box(low=np.zeros(state_dim).astype(float), high=np.ones(state_dim).astype(float))\n",
    "action_space = gym.spaces.Discrete(action_dim)\n",
    "\n",
    "env = MyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d1dc892-b799-4871-8c1e-80c39f0d3a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/opt/notobooks/ppo/log\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "curdir = Path().resolve()\n",
    "logdir = curdir / \"log\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "new_logger = configure(str(logdir), [\"stdout\", \"csv\"])\n",
    "\n",
    "model = PPO(policy=ActorCriticPolicy, env=env, verbose=1)\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "084196c2-c776-4a09-9b56-283f6595284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Eval num_timesteps=2048, episode_reward=-7.00 +/- 2.53\n",
      "Episode length: 11.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 4964     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4096, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | 11         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03332196 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | -0.0055    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.67       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 4.69       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 2458     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6144, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11          |\n",
      "|    mean_reward          | 11          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023235928 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 0.96     |\n",
      "| time/              |          |\n",
      "|    fps             | 2137     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8192, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 11       |\n",
      "|    mean_reward          | 11       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 8192     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.027937 |\n",
      "|    clip_fraction        | 0.664    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.861   |\n",
      "|    explained_variance   | -0.00204 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 2.83     |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | -0.0876  |\n",
      "|    value_loss           | 3.91     |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 1968     |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10240, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | 11         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06291794 |\n",
      "|    clip_fraction        | 0.495      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | -0.00278   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.42       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0706    |\n",
      "|    value_loss           | 4.27       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 1852     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12288, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | 11         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12308656 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | -0.00712   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2          |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    value_loss           | 5.3        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.78     |\n",
      "| time/              |          |\n",
      "|    fps             | 1808     |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14336, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11           |\n",
      "|    mean_reward          | 11           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017336288 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.196       |\n",
      "|    explained_variance   | -0.00404     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.61         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00631     |\n",
      "|    value_loss           | 7.09         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 1778     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16384, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11           |\n",
      "|    mean_reward          | 11           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010944388 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | -0.00299     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.36         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00562     |\n",
      "|    value_loss           | 8.05         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.82     |\n",
      "| time/              |          |\n",
      "|    fps             | 1773     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18432, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 11            |\n",
      "|    mean_reward          | 11            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00082986534 |\n",
      "|    clip_fraction        | 0.0183        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.136        |\n",
      "|    explained_variance   | -0.00428      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.44          |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00246      |\n",
      "|    value_loss           | 8.37          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.84     |\n",
      "| time/              |          |\n",
      "|    fps             | 1782     |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20480, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11           |\n",
      "|    mean_reward          | 11           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007068474 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | -0.00145     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.53         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00309     |\n",
      "|    value_loss           | 8.63         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 1779     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22528, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11           |\n",
      "|    mean_reward          | 11           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005651779 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0886      |\n",
      "|    explained_variance   | -0.00119     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    value_loss           | 8.72         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 1766     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24576, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 11            |\n",
      "|    mean_reward          | 11            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041117397 |\n",
      "|    clip_fraction        | 0.0111        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0723       |\n",
      "|    explained_variance   | -0.000825     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.78          |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.00151      |\n",
      "|    value_loss           | 8.91          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 1739     |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26624, episode_reward=11.00 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11           |\n",
      "|    mean_reward          | 11           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002961278 |\n",
      "|    clip_fraction        | 0.00928      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0573      |\n",
      "|    explained_variance   | -0.00204     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 8.98         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9      |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 1721     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0xffff3333c430>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.learn(total_timesteps=25000, eval_env=env, eval_freq=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc2f97cd-eb60-4c1d-8c47-11c7ce3761a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "i = 0\n",
    "actions = list()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, _ = env.step(action)\n",
    "    actions.append(action)\n",
    "    # env.render()\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e2728-4962-4807-8717-2b89351c3092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
